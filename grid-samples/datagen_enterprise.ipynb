{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelized Data Generation with GRID Enterprise\n",
    "\n",
    "This notebook demonstrates how to parallelize and scale data generation across your cluster using GRID Enterprise.\n",
    "\n",
    "This assumes you have installed all the dependencies and have initiated the sessions on your cluster.\n",
    "\n",
    "For more information, please visit [GRID Enterprise Documentation](https://docs.scaledfoundations.ai/grid_enterprise/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading resource configuration from /home/grid/.grid/resource_config.json...\n",
      "+-------------+---------------+\n",
      "| Node Name   | IP Address    |\n",
      "+=============+===============+\n",
      "| local       | localhost     |\n",
      "+-------------+---------------+\n",
      "| node1       | 20.83.236.157 |\n",
      "+-------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import os\n",
    "import json\n",
    "from typing import List\n",
    "from grid.sdk.manager import GRIDSessionManager  # Adjust import path if needed\n",
    "\n",
    "# Initialize GRIDSessionManager\n",
    "manager = GRIDSessionManager()\n",
    "# List nodes and store IPs in a list\n",
    "nodes = manager.list_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading resource configuration from /home/grid/.grid/resource_config.json...\n"
     ]
    }
   ],
   "source": [
    "resource_config = manager.load_resource_config()\n",
    "# Extract node information\n",
    "nodes_info = []\n",
    "for node_name, details in resource_config.get(\"resources\", {}).items():\n",
    "    node_info = {\n",
    "        \"node_name\": node_name,\n",
    "        \"ip\": details.get(\"ip\"),\n",
    "        \"username\": details.get(\"username\", \"\"),  # Default to empty string if missing\n",
    "        \"password\": details.get(\"password\", \"\")   \n",
    "    }\n",
    "    nodes_info.append(node_info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation for Multiple Environments\n",
    "\n",
    "In this example, we aim to generate data using the recording API and a simple script across multiple environments. To achieve parallelized runs, we’ll first create session configurations for each environment we want to process concurrently.\n",
    "\n",
    "## Steps\n",
    "\n",
    "1. **Define the Environments:** \n",
    "   We specify a list of environments (`env_list`) that we want to include in our data generation, such as `\"abandoned_factory\"`, `\"electric_central\"`, and `\"neighborhood\"`. This list can be modified to include other environments as required.\n",
    "\n",
    "2. **Set Up the Base Configuration Template:** \n",
    "   A base JSON structure (`base_json`) defines essential simulation settings, including:\n",
    "   - The `airgen` section, which configures environment-specific settings such as the `SimMode`, `VehicleType`, and `VehicleModel`.\n",
    "   - The `grid` section, which specifies entities for the simulation, including a robot named `\"airgen-drone\"`.\n",
    "\n",
    "Each environment in `env_list` will use this base template, and we’ll modify `env_name` for each to parallelize data generation runs across the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of environments\n",
    "env_list = [\"abandoned_factory\", \"electric_central\", \"neighborhood\"]  # Replace with actual environment names\n",
    "\n",
    "# Create the base JSON template\n",
    "base_json = {\n",
    "    \"airgen\": {\n",
    "        \"env_name\": \"neighborhood\",\n",
    "        \"geo\": False,\n",
    "        \"settings\": {\n",
    "            \"SimMode\": \"Multirotor\",\n",
    "            \"Vehicles\": {\n",
    "                \"Drone\": {\n",
    "                    \"VehicleType\": \"SimpleFlight\",\n",
    "                    \"VehicleModel\": \"Default\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"grid\": {\n",
    "        \"entities\": {\n",
    "            \"robot\": [{\"name\": \"airgen-drone\", \"kwargs\": {}}],\n",
    "            \"model\": []\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder created for storing session configs: /home/grid/sweep\n"
     ]
    }
   ],
   "source": [
    "# Create the 'sweep' folder if it doesn't already exist\n",
    "sweep_folder = os.path.expanduser(\"~/sweep\")\n",
    "os.makedirs(sweep_folder, exist_ok=True)\n",
    "print(f\"Folder created for storing session configs: {sweep_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Unique Session Configurations with Recording Setup\n",
    "\n",
    "To set up parallelized data generation across multiple environments, we create unique session configurations for each environment and define a separate recording folder for each session.\n",
    "\n",
    "## Process\n",
    "\n",
    "1. **Session ID Counter:**\n",
    "   We start with `session_id = 1` and increment it for each environment to ensure each session has a unique identifier.\n",
    "\n",
    "2. **Environment Loop:**\n",
    "   For each environment in `env_list`:\n",
    "   - A copy of the base configuration template (`base_json`) is modified to set the environment name (`env_name`).\n",
    "   - We add a unique recording configuration for each session, setting up a dedicated folder under `/tmp/recording/{session_id}/{env}` where data will be stored separately for each job.\n",
    "\n",
    "3. **Write Config Files:**\n",
    "   Each environment’s configuration is saved to a JSON file in the `sweep_folder` directory, ensuring each session has its own config file.\n",
    "\n",
    "This setup enables us to generate and store data independently for each environment, making it easy to manage data output for parallel jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session config created: /home/grid/sweep/session_1_abandoned_factory.json\n",
      "Session config created: /home/grid/sweep/session_2_electric_central.json\n",
      "Session config created: /home/grid/sweep/session_3_neighborhood.json\n"
     ]
    }
   ],
   "source": [
    "# Start the session ID counter\n",
    "session_id = 1\n",
    "\n",
    "# Loop over each environment to create session configs\n",
    "for env in env_list:\n",
    "    # Copy the base JSON and update it with environment-specific settings\n",
    "    config = base_json.copy()\n",
    "    config[\"airgen\"][\"env_name\"] = env\n",
    "    \n",
    "    # Add the recording configuration\n",
    "    config[\"airgen\"][\"settings\"][\"Recording\"] = {\n",
    "        \"RecordOnMove\": False,\n",
    "        \"RecordInterval\": 0.05,\n",
    "        \"Folder\": f\"/tmp/recording/{session_id}/{env}\",\n",
    "        \"Enabled\": False,\n",
    "        \"Cameras\": [\n",
    "            {\n",
    "                \"CameraName\": \"0\",\n",
    "                \"ImageType\": 0,\n",
    "                \"PixelsAsFloat\": False,\n",
    "                \"VehicleName\": \"\",\n",
    "                \"Compress\": True\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Write the configuration to a JSON file\n",
    "    config_file_path = os.path.join(sweep_folder, f\"session_{session_id}_{env}.json\")\n",
    "    with open(config_file_path, 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    \n",
    "    print(f\"Session config created: {config_file_path}\")\n",
    "    \n",
    "    # Increment session ID\n",
    "    session_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions\n",
    "\n",
    "- **_upload_via_sftp_async:** Asynchronously uploads files or directories to either a local or remote node.\n",
    "- **execute_command_async:** Executes a command asynchronously on a specified local or remote node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import shlex\n",
    "import asyncssh\n",
    "import subprocess\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "async def _upload_via_sftp_async(node_info, local_path, node_name, remote_folder):\n",
    "    \"\"\"\n",
    "    Asynchronously uploads a file via SFTP using asyncssh.\n",
    "    Handles local file upload separately if node_name is 'local'.\n",
    "    \"\"\"\n",
    "    if node_name == \"local\":\n",
    "        # Ensure remote_folder exists locally\n",
    "        os.makedirs(remote_folder, exist_ok=True)\n",
    "        \n",
    "        if os.path.isdir(local_path):\n",
    "            # Copy only the contents of the directory\n",
    "            for item in os.listdir(local_path):\n",
    "                s = os.path.join(local_path, item)\n",
    "                d = os.path.join(remote_folder, item)\n",
    "                if os.path.isdir(s):\n",
    "                    shutil.copytree(s, d, dirs_exist_ok=True)\n",
    "                else:\n",
    "                    shutil.copy2(s, d)\n",
    "        else:\n",
    "            # Copy single file directly\n",
    "            shutil.copy2(local_path, remote_folder)\n",
    "        \n",
    "        print(f\"Copied contents of {local_path} to {remote_folder} on local machine.\")\n",
    "    else:\n",
    "        # Remote upload using SFTP\n",
    "        remote_ip = node_info[\"ip\"]\n",
    "        username = node_info[\"username\"]\n",
    "        password = node_info[\"password\"]\n",
    "\n",
    "        try:\n",
    "            async with asyncssh.connect(remote_ip, username=username, password=password) as conn:\n",
    "                async with conn.start_sftp_client() as sftp:\n",
    "                    try:\n",
    "                        await sftp.mkdir(remote_folder)\n",
    "                    except asyncssh.SFTPError:\n",
    "                        pass  # Ignore error if folder already exists\n",
    "\n",
    "                    # Upload files or folder contents\n",
    "                    if os.path.isdir(local_path):\n",
    "                        for root, _, files in os.walk(local_path):\n",
    "                            remote_subfolder = os.path.join(remote_folder, os.path.relpath(root, local_path)).replace(\"\\\\\", \"/\")\n",
    "                            try:\n",
    "                                await sftp.mkdir(remote_subfolder)\n",
    "                            except asyncssh.SFTPError:\n",
    "                                pass  # Ignore if folder already exists\n",
    "                            for file in files:\n",
    "                                local_file_path = os.path.join(root, file)\n",
    "                                remote_file_path = os.path.join(remote_subfolder, file).replace(\"\\\\\", \"/\")\n",
    "                                await sftp.put(local_file_path, remote_file_path)\n",
    "                    else:\n",
    "                        remote_file_path = os.path.join(remote_folder, os.path.basename(local_path)).replace(\"\\\\\", \"/\")\n",
    "                        await sftp.put(local_path, remote_file_path)\n",
    "                    print(f\"Uploaded {local_path} to {node_name}:{remote_folder}\")\n",
    "        except asyncssh.SFTPError as e:\n",
    "            print(f\"Error uploading via SFTP to {node_name}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"General error uploading via SFTP to {node_name}: {e}\")\n",
    "        \n",
    "\n",
    "async def execute_command_async(command: str, node_info: dict, run_quiet: bool = False) -> str:\n",
    "    \"\"\"Execute a command asynchronously on a local or remote node.\"\"\"\n",
    "    node_name = node_info.get(\"node_name\")\n",
    "    ip = node_info.get(\"ip\")\n",
    "    username = node_info.get(\"username\")\n",
    "    password = node_info.get(\"password\")\n",
    "\n",
    "    if node_name == \"local\":\n",
    "        # Local execution using asyncio with subprocess\n",
    "        process = await asyncio.create_subprocess_exec(\n",
    "            *shlex.split(command),\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE\n",
    "        )\n",
    "        stdout, stderr = await process.communicate()\n",
    "\n",
    "        if stdout:\n",
    "            if not run_quiet:\n",
    "                print(stdout.decode().strip())\n",
    "            return stdout.decode().strip()\n",
    "        \n",
    "        if stderr:\n",
    "            error = stderr.decode().strip()\n",
    "            print(f\"Error: {error}\")\n",
    "            return f\"Error: {error}\"\n",
    "\n",
    "    else:\n",
    "        # Remote execution with asyncssh\n",
    "        try:\n",
    "            async with asyncssh.connect(ip, username=username, password=password) as conn:\n",
    "                result = await conn.run(command, check=True)\n",
    "                output = result.stdout.strip()\n",
    "                \n",
    "                if not run_quiet:\n",
    "                    print(f\"Output from {node_name}: {output}\")\n",
    "                \n",
    "                return output\n",
    "\n",
    "        except asyncssh.Error as e:\n",
    "            error_msg = f\"SSH Error on {node_name}: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            return error_msg\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Exception on {node_name}: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            return error_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Data Generation Across Nodes\n",
    "\n",
    "In this section, we create a job queue for each node to assign session configurations, distributing the data generation work across multiple nodes in parallel.\n",
    "\n",
    "1. **Session Config Distribution**: The function begins by reading session config files and dividing them equally across the available nodes. This means each node receives a portion of the configuration files to process.\n",
    "\n",
    "2. **Remote Setup and Execution**: \n",
    "   - For each node, we ensure a temporary folder (`/tmp/datagen_code/`) exists locally on the node, then upload the necessary code files. The code is then copied into the Docker container on that node.\n",
    "   - A session is started for each configuration file on each node. We also add a recording folder specific to each session within the container to keep generated data organized.\n",
    "\n",
    "3. **Parallel Execution**: Each node runs its assigned session configs sequentially. However, nodes themselves are working in parallel with other nodes to maximize throughput across the cluster.\n",
    "\n",
    "4. **Running the Script and Pausing**:\n",
    "   - After starting the session, a pause is added to ensure the session is fully initialized before proceeding.\n",
    "   - Next, the main data generation script runs inside the container.\n",
    "   - Finally, each session is stopped once completed, ensuring the node is ready for the next job.\n",
    "\n",
    "This approach effectively parallelizes the data generation tasks, distributing them across all nodes in the cluster, enhancing speed and scalability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def datagen_parallel(manager, session_settings_path, node_list, code_folder):\n",
    "    \"\"\"Simplified parallel data generation across nodes with session configs.\"\"\"\n",
    "\n",
    "    # Load session config files and distribute them to nodes equally\n",
    "    config_files = [file for file in os.listdir(session_settings_path) if file.endswith('.json') or file.endswith('.yml')]\n",
    "    num_nodes = len(node_list)\n",
    "    configs_per_node = {node['node_name']: config_files[i::num_nodes] for i, node in enumerate(node_list)}\n",
    "\n",
    "    # Define remote paths\n",
    "    local_tmp_folder = '/tmp/datagen_code/'\n",
    "    docker_tmp_folder = '/tmp/datagen_tmp_code/'\n",
    "    script_path = os.path.join('/tmp/datagen_tmp_code/datagen_code', 'drone_flight_test.py')\n",
    "    \n",
    "    async def process_node_sessions(node, configs):\n",
    "        \"\"\"Process each session config for a single node sequentially.\"\"\"\n",
    "        node_name = node[\"node_name\"]\n",
    "        node_ip = node[\"ip\"]\n",
    "        container_name = \"grid_core\"\n",
    "\n",
    "        # Ensure local folder exists and upload code to node’s local tmp folder\n",
    "        print(f\"Preparing datagen code folder on {node_name}...\")\n",
    "        await execute_command_async(f\"mkdir -p {local_tmp_folder}\", node)\n",
    "        await _upload_via_sftp_async(node,code_folder, node_name, local_tmp_folder)\n",
    "        \n",
    "        # Copy code to the container’s tmp directory\n",
    "        print(f\"Copying code folder to {docker_tmp_folder} in {container_name} on {node_name}...\")\n",
    "        await execute_command_async(f\"docker cp {local_tmp_folder} {container_name}:{docker_tmp_folder}\", node)\n",
    "\n",
    "        for config_file in configs:\n",
    "            session_id = os.path.splitext(config_file)[0]\n",
    "            session_config_path = os.path.join(session_settings_path, config_file)\n",
    "\n",
    "            # Load config and create recording folder in container\n",
    "            config_data = manager.create_config(session_config_path, session_id)\n",
    "            recording_folder = config_data[\"session\"][\"airgen\"][\"settings\"][\"Recording\"][\"Folder\"]\n",
    "            \n",
    "            # Ensure recording folder exists in the container\n",
    "            await execute_command_async(f\"docker exec grid_service mkdir -p {recording_folder}\", node)\n",
    "            \n",
    "            # Start the session\n",
    "            print(f\"Starting session {session_id} on {node_name}...\")\n",
    "            await manager.start_session(session_id=session_id, session_config_file_path=session_config_path, node_ip=node_ip)\n",
    "            \n",
    "            # Pause asynchronously for a specified duration (e.g., 5 seconds)\n",
    "            pause_duration = 30  # Adjust the duration as needed\n",
    "            print(f\"Pausing for {pause_duration} seconds...\")\n",
    "            await asyncio.sleep(pause_duration)\n",
    "    \n",
    "\n",
    "            # Run the flight test script asynchronously in background\n",
    "            print(f\"Running script {script_path} for session {session_id} on {node_name}...\")\n",
    "            await execute_command_async(f\"docker exec grid_core python {script_path}\", node)\n",
    "            \n",
    "            # Stop the session\n",
    "            print(f\"Stopping session {session_id} on {node_name}...\")\n",
    "            await manager.stop_session(session_id=session_id)\n",
    "    \n",
    "    # Gather tasks for all nodes and execute them concurrently\n",
    "    await asyncio.gather(*(process_node_sessions(node, configs_per_node[node[\"node_name\"]]) for node in node_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the data generation process, you will see that recording folders for each session are created inside the `grid_service` container. These folders store the recording data specific to each session, allowing you to easily manage and access the output generated for each environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing datagen code folder on local...\n",
      "Preparing datagen code folder on node1...\n",
      "Copied contents of /home/grid/airgenpractise/AirgenPractise/sample_code to /tmp/datagen_code/ on local machine.\n",
      "Copying code folder to /tmp/datagen_tmp_code/ in grid_core on local...\n",
      "Starting session session_1_abandoned_factory on local...\n",
      "Starting session session_1_abandoned_factory ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node1: \n",
      "Uploaded /home/grid/airgenpractise/AirgenPractise/sample_code to node1:/tmp/datagen_code/\n",
      "Copying code folder to /tmp/datagen_tmp_code/ in grid_core on node1...\n",
      "Output from node1: \n",
      "Output from node1: \n",
      "Starting session session_2_electric_central on node1...\n",
      "Starting session session_2_electric_central ...\n",
      "response: Starting session...\n",
      "response: Downloading resources..\n",
      "response_end: Session has been started successfully\n",
      "Session started successfully.\n",
      "Pausing for 30 seconds...\n",
      "response: Starting session...\n",
      "response: Downloading resources..\n",
      "response_end: Session has been started successfully\n",
      "Session started successfully.\n",
      "Pausing for 30 seconds...\n",
      "Running script /tmp/datagen_tmp_code/datagen_code/drone_flight_test.py for session session_1_abandoned_factory on local...\n",
      "Running script /tmp/datagen_tmp_code/datagen_code/drone_flight_test.py for session session_2_electric_central on node1...\n",
      "Connecting to AirGen simulator...\n",
      "Connected!\n",
      "Client Ver:1 (Min Req: 1), Server Ver:1 (Min Req: 1)\n",
      "\n",
      "Connected!\n",
      "Enabling API control...\n",
      "Arming the drone...\n",
      "Starting recording...\n",
      "Taking off...\n",
      "Moving forward by 10 meters...\n",
      "Stopping recording...\n",
      "Landing...\n",
      "Disabling API control...\n",
      "Flight complete.\n",
      "Stopping session session_1_abandoned_factory on local...\n",
      "Stopping session session_1_abandoned_factory ...\n",
      "Output from node1: Connecting to AirGen simulator...\n",
      "Connected!\n",
      "Client Ver:1 (Min Req: 1), Server Ver:1 (Min Req: 1)\n",
      "\n",
      "Connected!\n",
      "Enabling API control...\n",
      "Arming the drone...\n",
      "Starting recording...\n",
      "Taking off...\n",
      "Moving forward by 10 meters...\n",
      "Stopping recording...\n",
      "Landing...\n",
      "Disabling API control...\n",
      "Flight complete.\n",
      "Stopping session session_2_electric_central on node1...\n",
      "Stopping session session_2_electric_central ...\n",
      "Session stopped successfully.\n",
      "Starting session session_3_neighborhood on local...\n",
      "Starting session session_3_neighborhood ...\n",
      "Session stopped successfully.\n",
      "response: Starting session...\n",
      "response: Downloading resources..\n",
      "response_end: Session has been started successfully\n",
      "Session started successfully.\n",
      "Pausing for 30 seconds...\n",
      "Running script /tmp/datagen_tmp_code/datagen_code/drone_flight_test.py for session session_3_neighborhood on local...\n",
      "Connecting to AirGen simulator...\n",
      "Connected!\n",
      "Client Ver:1 (Min Req: 1), Server Ver:1 (Min Req: 1)\n",
      "\n",
      "Connected!\n",
      "Enabling API control...\n",
      "Arming the drone...\n",
      "Starting recording...\n",
      "Taking off...\n",
      "Moving forward by 10 meters...\n",
      "Stopping recording...\n",
      "Landing...\n",
      "Disabling API control...\n",
      "Flight complete.\n",
      "Stopping session session_3_neighborhood on local...\n",
      "Stopping session session_3_neighborhood ...\n",
      "Session stopped successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the paths for the session settings and code folder\n",
    "session_settings_path = os.path.expanduser(\"~/sweep\")\n",
    "code_folder = \"/tmp/sample_code\"  # Replace with your actual code folder path\n",
    "\n",
    "# Now run `datagen_parallel`\n",
    "await datagen_parallel(manager, session_settings_path, nodes_info, code_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
