{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this sample notebook demonstrates the various capabilities \n",
    "# of the GRID platform that will allow the user to rapidly solve a \n",
    "# simple use case of moving towards a sphere and asking questions\n",
    "# about it. This could be extrapolated to scenatios like warehouse\n",
    "# supervision , fault detection in electric lines , etc.\n",
    "\n",
    "from grid.robot.airgen_drone import AirGenDrone\n",
    "airgen_drone_0 = AirGenDrone(**{'geo': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid.model.perception.vlm.llava import LLaVA\n",
    "from grid.model.perception.detection.gdino import GroundingDINO\n",
    "from grid.model.navigation.visualservoing import VisualServoing\n",
    "detection_model = GroundingDINO()\n",
    "qa_model = LLaVA()\n",
    "servoing_model = VisualServoing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import airgen\n",
    "import numpy as np\n",
    "airgen_drone_0 = AirGenDrone(**{'geo': False})\n",
    "\n",
    "client = airgen_drone_0.client\n",
    "client.reset()\n",
    "client.confirmConnection()\n",
    "client.enableApiControl(True)\n",
    "client.armDisarm(True)\n",
    "\n",
    "# Take off\n",
    "client.takeoffAsync().join()\n",
    "\n",
    "client.moveByVelocityAsync(5, 0, 0, 5).join()\n",
    "client.moveByVelocityAsync(0, 2, 0, 5).join()\n",
    "\n",
    "\n",
    "# Capture image from AirGen\n",
    "def capture_images(client):\n",
    "    responses = client.simGetImages([\n",
    "        airgen.ImageRequest(\"0\", airgen.ImageType.Scene, False, False),\n",
    "        airgen.ImageRequest(\"0\", airgen.ImageType.DepthPerspective, True, False)\n",
    "    ])\n",
    "    rgb_image = np.frombuffer(responses[0].image_data_uint8, dtype=np.uint8).reshape(responses[0].height, responses[0].width, 3)\n",
    "    depth_image = np.array(responses[1].image_data_float, dtype=np.float32).reshape(responses[1].height, responses[1].width)\n",
    "    return rgb_image, depth_image\n",
    "\n",
    "# Calculate distance to the object\n",
    "def calculate_distance(depth_image, bounding_box):\n",
    "    x_min, y_min, x_max, y_max = bounding_box\n",
    "    depth_region = depth_image[int(y_min):int(y_max), int(x_min):int(x_max)]\n",
    "    distance = np.min(depth_region)  # minimum distance within the bounding box\n",
    "    return distance\n",
    "\n",
    "# Sample use case\n",
    "def move_towards_sphere_and_ask_color(client, detection_model, servoing_model, qa_model, target_object, question, stop_distance=10.0, iterations=30):\n",
    "    for i in range(iterations):\n",
    "        # Step 1: Capture images\n",
    "        rgb_image, depth_image = capture_images(client)\n",
    "        \n",
    "        # Step 2: Detect objects\n",
    "        bounding_boxes, phrases = detection_model.detect_object(rgb_image, target_object)\n",
    "        \n",
    "        if len(bounding_boxes) == 0:\n",
    "            print(f\"No objects detected for prompt: {target_object}\")\n",
    "            continue\n",
    "        \n",
    "        # Step 3: Select the first detected spherical object\n",
    "        target_bbox = bounding_boxes[0]\n",
    "        target_phrase = phrases[0]\n",
    "        \n",
    "        # Calculate the center of the bounding box\n",
    "        x_center = (target_bbox[0] + target_bbox[2]) / 2\n",
    "        y_center = (target_bbox[1] + target_bbox[3]) / 2\n",
    "        target_image_coord = (x_center, y_center)\n",
    "        \n",
    "        # Calculate the distance to the target object\n",
    "        distance = calculate_distance(depth_image, target_bbox)\n",
    "        print(f\"Iteration {i+1}: Distance to target: {distance:.2f} meters\")\n",
    "        \n",
    "        # Step 4: Check if the drone is within the stop distance\n",
    "        if distance <= stop_distance:\n",
    "            print(f\"Stopping: Within {stop_distance} meters of the target object\")\n",
    "            break\n",
    "        \n",
    "        # Step 5: Move drone to the target\n",
    "        camera_param = {\n",
    "            \"width\": rgb_image.shape[1],\n",
    "            \"height\": rgb_image.shape[0],\n",
    "            \"fov\": 90.0,\n",
    "            \"camera_orientation_euler_pry\": (0.0, 0.0, 0.0)\n",
    "        }\n",
    "        delta_yaw, velocity_vector = servoing_model.moveDrone2Target(target_image_coord, camera_param)\n",
    "        client.moveByVelocityAsync(velocity_vector[0], velocity_vector[1], velocity_vector[2], 1).join()\n",
    "        \n",
    "        print(f\"Iteration {i+1}: Moving towards {target_phrase}\")\n",
    "        \n",
    "    # Step 6: Capture image again to ask the question about the sphere's color\n",
    "    rgb_image, _ = capture_images(client)\n",
    "    \n",
    "    # Step 7: Answer question about the color of the sphere\n",
    "    answer = qa_model.run(rgb_image, question)\n",
    "    print(f\"Answer to '{question}': {answer}\")\n",
    "\n",
    "# Use case parameters\n",
    "target_object = \"sphere\"\n",
    "question = \"What is the color of the sphere?\"\n",
    "\n",
    "# Run the workflow to move towards the sphere and ask about its color\n",
    "move_towards_sphere_and_ask_color(client, detection_model, servoing_model, qa_model, target_object, question, stop_distance=5.0, iterations=30)\n",
    "\n",
    "# Land the drone\n",
    "client.landAsync().join()\n",
    "client.armDisarm(False)\n",
    "client.enableApiControl(False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
