{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172aa063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the humanoid simulation\n",
    "humanoidisaacsim_0.run()\n",
    "\n",
    "# Import necessary utilities for velocity control, logging, and timing\n",
    "from grid.utils.types import Velocity\n",
    "from grid.utils import log\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ea146d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotate the robot to get a better view of the scene, while capturing and logging images for 5 seconds.\n",
    "\n",
    "start_time = time.time()\n",
    "while time.time() - start_time < 5:\n",
    "    # Turn the robot by applying a small angular velocity (in this case, rotating about the Z-axis)\n",
    "    humanoidisaacsim_0.locomotion.moveByVelocity(Velocity(0, 0, 0), Velocity(0, 0, -2))\n",
    "\n",
    "    # Retrieve the RGB and Depth images from the simulation\n",
    "    rgb_img = humanoidisaacsim_0.locomotion.getImage()\n",
    "    depth_img = humanoidisaacsim_0.locomotion.getImage(\"camera_depth_0\")\n",
    "\n",
    "    # Log the RGB image if available\n",
    "    if rgb_img.data is not None:\n",
    "        log(\"rgb_img\", rgb_img)\n",
    "    else:\n",
    "        print(\"RGB image is none\")\n",
    "\n",
    "    # Log the depth image if available; scale the depth data for visualization purposes\n",
    "    if depth_img.data is not None:\n",
    "        depth_img.data = depth_img.data * 255\n",
    "        log(\"depth_img\", depth_img)\n",
    "    else:\n",
    "        print(\"Depth image is none\")\n",
    "    \n",
    "    time.sleep(0.2)\n",
    "\n",
    "# Stop the robot's movement after the loop completes\n",
    "humanoidisaacsim_0.locomotion.moveByVelocity(Velocity(0, 0, 0), Velocity(0, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a611c187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and initialize perception models for segmentation, depth estimation, and vision-language processing.\n",
    "\n",
    "from grid.model.perception.segmentation.oneformer import OneFormer \n",
    "from grid.model.perception.depth.depth_anything_v2 import DepthAnything_V2 \n",
    "from grid.model.perception.vlm.llava import LLaVA\n",
    "\n",
    "# Create model instances\n",
    "seg_model = OneFormer()\n",
    "depth_model = DepthAnything_V2()\n",
    "vlm_model = LLaVA()\n",
    "\n",
    "# Retrieve the latest RGB image from the simulation for processing\n",
    "rgb = humanoidisaacsim_0.getImage()\n",
    "\n",
    "# Import rerun for logging the perception outputs\n",
    "import rerun as rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2110578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the segmentation model with a \"panoptic\" prompt and log the result.\n",
    "seg = seg_model.run(rgb_img.data, \"panoptic\")\n",
    "rr.log(\"humanoid/segmentation\", rr.SegmentationImage(seg))\n",
    "\n",
    "# Run the depth estimation model and log the result.\n",
    "depth = depth_model.run(rgb_img.data)\n",
    "rr.log(\"humanoid/depth\", rr.DepthImage(depth))\n",
    "\n",
    "# Run the vision-language model to generate a caption describing the scene.\n",
    "caption = vlm_model.run(rgb_img.data, \"Describe what you see.\")\n",
    "print(caption)\n",
    "\n",
    "# You can modify the caption prompt, or ask a question, and rerun this cell to see updated results."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
